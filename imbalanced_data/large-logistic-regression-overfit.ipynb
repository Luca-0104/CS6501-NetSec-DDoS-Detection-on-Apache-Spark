{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b8c8f7-cfb7-4346-b3e0-0208cd4c1ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/spark-3.3.1-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.3.1-bin-hadoop3')\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03315ce3-217c-4833-b790-1d4fe14275fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/27 01:04:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# The entry point into all functionality in Spark is the SparkSession class.\n",
    "spark = (SparkSession\n",
    "\t.builder\n",
    "\t.appName(\"DS5110/CS5501: DDoS detection project\")\n",
    "\t.master(\"spark://172.31.24.89:7077\")\n",
    "\t.config(\"spark.executor.memory\", \"1024M\")\n",
    "\t.getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db79c6-4b87-4bbe-a47c-ebcb5bcec8ac",
   "metadata": {},
   "source": [
    "### Load csv data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57db617d-c774-4fb2-b975-ca8e2ea82251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (spark.read\n",
    "         .format(\"csv\")\n",
    "         .option(\"inferSchema\", True)\n",
    "         .option(\"header\", True)\n",
    "         .load(\"hdfs://172.31.24.89:9000/02-20-2018.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f31ccbaf-225f-4052-a2f3-4dca93f85d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560c5d98-7f58-48ef-a258-e654bf235f31",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edd9fa31-9648-45c8-a1fa-2f5f044d13f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "#drop unused columns, drop Nan values, convert string labels to integer\n",
    "df = df.drop('Timestamp', 'Flow ID')\n",
    "df = df.na.drop()\n",
    "df = df.withColumn(\"Label\", when(col(\"Label\") == \"Benign\", 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10794986-3b8a-4eb9-ba6a-006ddf4e5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b62bb-ac62-46db-8565-b751fe1c8b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate features and label columns\n",
    "\n",
    "# Replace missing values in 'Label' with the most common value (mode)\n",
    "# mode = labels.groupBy(\"Label\").count().orderBy(\"count\", ascending=False).first()[\"Label\"]\n",
    "# labels = labels.na.fill(value=mode, subset=[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c8782-0dca-4467-8857-734e5296268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Src IP\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79999730-13e0-4352-9a0c-af8da6fd742c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import LongType\n",
    "import socket\n",
    "import struct\n",
    "\n",
    "# Define a UDF to convert IP to integer\n",
    "def ip_to_int(ip):\n",
    "    if ip:\n",
    "        return struct.unpack(\"!I\", socket.inet_aton(ip))[0]\n",
    "    return None\n",
    "\n",
    "# Register the UDF\n",
    "ip_to_int_udf = F.udf(ip_to_int, LongType())\n",
    "\n",
    "# Apply the UDF to transform the IP columns\n",
    "df = df.withColumn('Src IP Int', ip_to_int_udf(F.col('Src IP')))\n",
    "df = df.withColumn('Dst IP Int', ip_to_int_udf(F.col('Dst IP')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd4b43c7-3610-4358-be9e-90f8f126dd63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/27 01:06:46 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+-----------+-----------+-------------+------------+------------+------------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-------------+------------+----------------+----------------+--------------+--------------+----------------+--------------+--------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+---------+-----------+--------+--------+-----+----------+----------+\n",
      "|        Src IP|Src Port|      Dst IP|Dst Port|Protocol|Flow Duration|Tot Fwd Pkts|Tot Bwd Pkts|TotLen Fwd Pkts|TotLen Bwd Pkts|Fwd Pkt Len Max|Fwd Pkt Len Min|Fwd Pkt Len Mean|Fwd Pkt Len Std|Bwd Pkt Len Max|Bwd Pkt Len Min|Bwd Pkt Len Mean|Bwd Pkt Len Std|Flow Byts/s|Flow Pkts/s|Flow IAT Mean|Flow IAT Std|Flow IAT Max|Flow IAT Min|Fwd IAT Tot|Fwd IAT Mean|Fwd IAT Std|Fwd IAT Max|Fwd IAT Min|Bwd IAT Tot|Bwd IAT Mean|Bwd IAT Std|Bwd IAT Max|Bwd IAT Min|Fwd PSH Flags|Bwd PSH Flags|Fwd URG Flags|Bwd URG Flags|Fwd Header Len|Bwd Header Len| Fwd Pkts/s| Bwd Pkts/s|Pkt Len Min|Pkt Len Max|Pkt Len Mean|Pkt Len Std|Pkt Len Var|FIN Flag Cnt|SYN Flag Cnt|RST Flag Cnt|PSH Flag Cnt|ACK Flag Cnt|URG Flag Cnt|CWE Flag Count|ECE Flag Cnt|Down/Up Ratio|Pkt Size Avg|Fwd Seg Size Avg|Bwd Seg Size Avg|Fwd Byts/b Avg|Fwd Pkts/b Avg|Fwd Blk Rate Avg|Bwd Byts/b Avg|Bwd Pkts/b Avg|Bwd Blk Rate Avg|Subflow Fwd Pkts|Subflow Fwd Byts|Subflow Bwd Pkts|Subflow Bwd Byts|Init Fwd Win Byts|Init Bwd Win Byts|Fwd Act Data Pkts|Fwd Seg Size Min|Active Mean|Active Std|Active Max|Active Min|Idle Mean|   Idle Std|Idle Max|Idle Min|Label|Src IP Int|Dst IP Int|\n",
      "+--------------+--------+------------+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+-----------+-----------+-------------+------------+------------+------------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-------------+------------+----------------+----------------+--------------+--------------+----------------+--------------+--------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+---------+-----------+--------+--------+-----+----------+----------+\n",
      "|94.231.103.172|   45498|172.31.69.25|      22|       6|       888751|          11|          11|         1249.0|         1969.0|          736.0|            0.0|     113.5454545|    220.8960677|          976.0|            0.0|           179.0|    364.1864907|3620.811678|24.75383994|  42321.47619| 47851.73578|    101609.0|        14.0|   888751.0|     88875.1|49295.73551|   140273.0|       14.0|   788197.0|     78819.7|55863.18804|   139285.0|       72.0|            0|            0|            0|            0|           360|           360|12.37691997|12.37691997|        0.0|      976.0| 139.9130435| 290.633933|  84468.083|           0|           0|           0|           1|           0|           0|             0|           0|          1.0| 146.2727273|     113.5454545|           179.0|             0|             0|               0|             0|             0|               0|              11|            1249|              11|            1969|            14600|              233|                7|              32|        0.0|       0.0|       0.0|       0.0|      0.0|        0.0|     0.0|     0.0|    1|1592223660|2887730457|\n",
      "|       8.6.0.1|       0|     8.0.6.4|       0|       0|    112642816|           3|           0|            0.0|            0.0|            0.0|            0.0|             0.0|            0.0|            0.0|            0.0|             0.0|            0.0|        0.0|0.026632857|       5.63E7| 7.071067812|      5.63E7|      5.63E7|     1.13E8|      5.63E7|7.071067812|     5.63E7|     5.63E7|        0.0|         0.0|        0.0|        0.0|        0.0|            0|            0|            0|            0|             0|             0|0.026632857|        0.0|        0.0|        0.0|         0.0|        0.0|        0.0|           0|           0|           0|           0|           0|           0|             0|           0|          0.0|         0.0|             0.0|             0.0|             0|             0|               0|             0|             0|               0|               3|               0|               0|               0|               -1|               -1|                0|               0|        0.0|       0.0|       0.0|       0.0|   5.63E7|7.071067812|  5.63E7|  5.63E7|    1| 134610945| 134219268|\n",
      "+--------------+--------+------------+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+-----------+-----------+-------------+------------+------------+------------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-------------+------------+----------------+----------------+--------------+--------------+----------------+--------------+--------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+---------+-----------+--------+--------+-----+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad84bdbe-e8ce-4318-988b-ad63e3e35e22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label = df.select('Label').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a24a951d-96a4-44df-a397-b678d84b2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop('Src IP', 'Dst IP')\n",
    "# label = df.select('Label')\n",
    "# features = df.drop('Label')\n",
    "label = df.columns[-3]\n",
    "features= [col for col in df.columns if col != label]\n",
    "# label.show()\n",
    "# features.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d02c7ab-a084-4933-bad9-de486075a305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+-----------+-----------+-------------+------------+------------+------------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-------------+------------+----------------+----------------+--------------+--------------+----------------+--------------+--------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+---------+-----------+--------+--------+-----+----------+----------+\n",
      "|Src Port|Dst Port|Protocol|Flow Duration|Tot Fwd Pkts|Tot Bwd Pkts|TotLen Fwd Pkts|TotLen Bwd Pkts|Fwd Pkt Len Max|Fwd Pkt Len Min|Fwd Pkt Len Mean|Fwd Pkt Len Std|Bwd Pkt Len Max|Bwd Pkt Len Min|Bwd Pkt Len Mean|Bwd Pkt Len Std|Flow Byts/s|Flow Pkts/s|Flow IAT Mean|Flow IAT Std|Flow IAT Max|Flow IAT Min|Fwd IAT Tot|Fwd IAT Mean|Fwd IAT Std|Fwd IAT Max|Fwd IAT Min|Bwd IAT Tot|Bwd IAT Mean|Bwd IAT Std|Bwd IAT Max|Bwd IAT Min|Fwd PSH Flags|Bwd PSH Flags|Fwd URG Flags|Bwd URG Flags|Fwd Header Len|Bwd Header Len| Fwd Pkts/s| Bwd Pkts/s|Pkt Len Min|Pkt Len Max|Pkt Len Mean|Pkt Len Std|Pkt Len Var|FIN Flag Cnt|SYN Flag Cnt|RST Flag Cnt|PSH Flag Cnt|ACK Flag Cnt|URG Flag Cnt|CWE Flag Count|ECE Flag Cnt|Down/Up Ratio|Pkt Size Avg|Fwd Seg Size Avg|Bwd Seg Size Avg|Fwd Byts/b Avg|Fwd Pkts/b Avg|Fwd Blk Rate Avg|Bwd Byts/b Avg|Bwd Pkts/b Avg|Bwd Blk Rate Avg|Subflow Fwd Pkts|Subflow Fwd Byts|Subflow Bwd Pkts|Subflow Bwd Byts|Init Fwd Win Byts|Init Bwd Win Byts|Fwd Act Data Pkts|Fwd Seg Size Min|Active Mean|Active Std|Active Max|Active Min|Idle Mean|   Idle Std|Idle Max|Idle Min|Label|Src IP Int|Dst IP Int|\n",
      "+--------+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+-----------+-----------+-------------+------------+------------+------------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-------------+------------+----------------+----------------+--------------+--------------+----------------+--------------+--------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+---------+-----------+--------+--------+-----+----------+----------+\n",
      "|   45498|      22|       6|       888751|          11|          11|         1249.0|         1969.0|          736.0|            0.0|     113.5454545|    220.8960677|          976.0|            0.0|           179.0|    364.1864907|3620.811678|24.75383994|  42321.47619| 47851.73578|    101609.0|        14.0|   888751.0|     88875.1|49295.73551|   140273.0|       14.0|   788197.0|     78819.7|55863.18804|   139285.0|       72.0|            0|            0|            0|            0|           360|           360|12.37691997|12.37691997|        0.0|      976.0| 139.9130435| 290.633933|  84468.083|           0|           0|           0|           1|           0|           0|             0|           0|          1.0| 146.2727273|     113.5454545|           179.0|             0|             0|               0|             0|             0|               0|              11|            1249|              11|            1969|            14600|              233|                7|              32|        0.0|       0.0|       0.0|       0.0|      0.0|        0.0|     0.0|     0.0|    1|1592223660|2887730457|\n",
      "|       0|       0|       0|    112642816|           3|           0|            0.0|            0.0|            0.0|            0.0|             0.0|            0.0|            0.0|            0.0|             0.0|            0.0|        0.0|0.026632857|       5.63E7| 7.071067812|      5.63E7|      5.63E7|     1.13E8|      5.63E7|7.071067812|     5.63E7|     5.63E7|        0.0|         0.0|        0.0|        0.0|        0.0|            0|            0|            0|            0|             0|             0|0.026632857|        0.0|        0.0|        0.0|         0.0|        0.0|        0.0|           0|           0|           0|           0|           0|           0|             0|           0|          0.0|         0.0|             0.0|             0.0|             0|             0|               0|             0|             0|               0|               3|               0|               0|               0|               -1|               -1|                0|               0|        0.0|       0.0|       0.0|       0.0|   5.63E7|7.071067812|  5.63E7|  5.63E7|    1| 134610945| 134219268|\n",
      "+--------+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+-----------+-----------+-------------+------------+------------+------------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-------------+------------+----------------+----------------+--------------+--------------+----------------+--------------+--------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+---------+-----------+--------+--------+-----+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c20b459-47c2-4f57-9c2b-3dc2bf007783",
   "metadata": {},
   "source": [
    "### Feature selection using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a08ab608-20bc-4d77-b139-54564e2212cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "493e273b-b71b-4df4-be90-4e32d839b3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:==============>                                          (8 + 4) / 31]\r"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "pca = PCA(k=10, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")  # Reduce to 50 principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb523b8-b25f-4e1c-a17a-a34fa02b8022",
   "metadata": {},
   "source": [
    "### Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003037c4-78f3-453e-ab3c-b23ce6de22d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = pipeline.fit(df)\n",
    "# data = model.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aea0cab8-4cfd-402c-a2ef-349972317901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:==================>                                     (10 + 4) / 31]\r"
     ]
    }
   ],
   "source": [
    "# Select the features vector and the label column\n",
    "# data = data.select(\"pcaFeatures\", label)\n",
    "# Split the data into training and test sets\n",
    "trainData, testData = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f2e85f7-0043-4e78-baf6-b8c4c393cdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o580.fit.\n: breeze.linalg.NotConvergedException: \n\tat breeze.linalg.svd$.breeze$linalg$svd$$doSVD_Double(svd.scala:120)\n\tat breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:36)\n\tat breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:35)\n\tat breeze.generic.UFunc.apply(UFunc.scala:46)\n\tat breeze.generic.UFunc.apply$(UFunc.scala:45)\n\tat breeze.linalg.svd$.apply(svd.scala:21)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:501)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:65)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:64)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m lr \u001b[38;5;241m=\u001b[39m LogisticRegression(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpcaFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39mlabel, maxIter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, regParam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, elasticNetParam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m      5\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[assembler, scaler, pca, lr])\n\u001b[0;32m----> 6\u001b[0m lrModel \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainData\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o580.fit.\n: breeze.linalg.NotConvergedException: \n\tat breeze.linalg.svd$.breeze$linalg$svd$$doSVD_Double(svd.scala:120)\n\tat breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:36)\n\tat breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:35)\n\tat breeze.generic.UFunc.apply(UFunc.scala:46)\n\tat breeze.generic.UFunc.apply$(UFunc.scala:45)\n\tat breeze.linalg.svd$.apply(svd.scala:21)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:501)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:65)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:64)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "# Define and fit the Logistic Regression model\n",
    "import time\n",
    "start_time = time.time()\n",
    "lr = LogisticRegression(featuresCol=\"pcaFeatures\", labelCol=label, maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "pipeline = Pipeline(stages=[assembler, scaler, pca, lr])\n",
    "lrModel = pipeline.fit(trainData)\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e1141-e670-4c98-8dbe-a970de5e1ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8bbf4c-e8b6-4a93-b7ac-3530aa7ac9a0",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588a81ba-2f04-48f2-a2f7-0a1df755742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Label\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"precisionByLabel\")\n",
    "\n",
    "# You can specify the label here if you have multiple classes and you are interested in one specific class\n",
    "precision = precision_evaluator.evaluate(predictions)\n",
    "\n",
    "# Evaluator for recall\n",
    "recall_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"recallByLabel\")\n",
    "\n",
    "# As with precision, specify the label for recall calculation if needed\n",
    "recall = recall_evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e730b-5cc2-450b-9dd3-64b25e767216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
